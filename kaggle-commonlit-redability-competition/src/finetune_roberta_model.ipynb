{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune-roberta-model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNZyIcEfLG0GZowhSuvTWyF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9S8G7MUs2Q-",
        "outputId": "df60eccf-243f-41a8-c8e5-a75fb4c2c391"
      },
      "source": [
        "!pip install accelerate"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pyaml>=20.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (20.4.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.9.0+cu102)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=20.4.0->accelerate) (3.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9AfVwOquof6",
        "outputId": "085251a8-d666-4a98-d3b0-12c333f7cd77"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILoNsSja0eZD",
        "outputId": "89eb567e-efcd-4b47-8b95-6c05b5ba0906"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "pdnJCKB80h4T",
        "outputId": "96352c10-a5ff-462f-907b-b735e3532e76"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-886243cc-f516-4060-9b47-3e259ab75433\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-886243cc-f516-4060-9b47-3e259ab75433\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPJP05EXZ7Yv",
        "outputId": "bbf55067-3a15-47ae-dfb2-015c60a58199"
      },
      "source": [
        "# Here I'll use the kaggle dataset from Commolit Readability Competition https://www.kaggle.com/c/commonlitreadabilityprize/data\n",
        "# At first let me Mount my drive to get the dataset\n",
        "HOME_PATH = \"/content\"\n",
        "%cd \"$HOME_PATH\"\n",
        "from google.colab import drive \n",
        "drive.mount(\"/content/drive\")\n",
        "# Kaggle dataset path\n",
        "KAGGLE_PATH = \"/content/drive/My Drive/Data Science/data/\"\n",
        "DATASET_PATH = \"/content/drive/My Drive/Data Science/data/commonlitreadabilityprize/\""
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X606FlgO0vYh"
      },
      "source": [
        "#!mkdir ~/.kaggle\n",
        "!cp \"/content/drive/My Drive/Data Science/data/kaggle.json\" ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0W9ZiZR0ynY",
        "outputId": "67e33e30-5e4a-43f6-91f3-4eeabbc97586"
      },
      "source": [
        "!kaggle competitions download -c commonlitreadabilityprize"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading train.csv.zip to /content\n",
            "  0% 0.00/1.13M [00:00<?, ?B/s]\n",
            "100% 1.13M/1.13M [00:00<00:00, 79.1MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/108 [00:00<?, ?B/s]\n",
            "100% 108/108 [00:00<00:00, 104kB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/6.79k [00:00<?, ?B/s]\n",
            "100% 6.79k/6.79k [00:00<00:00, 6.55MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV3a_aXA01AE",
        "outputId": "df286973-6e3b-41d1-bf85-819800b2be5d"
      },
      "source": [
        "!mkdir input\n",
        "!unzip train.csv.zip -d input\n",
        "#!unzip file.zip -d input\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘input’: File exists\n",
            "Archive:  train.csv.zip\n",
            "replace input/train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKI339HG0wBr"
      },
      "source": [
        ""
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-Vlybp2A5Am",
        "outputId": "c615f485-32dd-4b3a-9ebc-347ee4fd5d27"
      },
      "source": [
        "!pip install colorama"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1iuURkus3M_"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from transformers import (AutoModel,AutoConfig,\n",
        "                          AutoTokenizer,get_cosine_schedule_with_warmup)\n",
        "from colorama import Fore, Back, Style\n",
        "r_ = Fore.RED\n",
        "b_ = Fore.BLUE\n",
        "c_ = Fore.CYAN\n",
        "g_ = Fore.GREEN\n",
        "y_ = Fore.YELLOW\n",
        "m_ = Fore.MAGENTA\n",
        "sr_ = Style.RESET_ALL\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q53w9z2umJJ"
      },
      "source": [
        "train_data = pd.read_csv('input/train.csv')\n",
        "test_data = pd.read_csv('input/test.csv')\n",
        "#sample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n",
        "\n",
        "train_data['excerpt'] = train_data['excerpt'].apply(lambda x: x.replace('\\n',''))\n",
        "\n",
        "num_bins = int(np.floor(1 + np.log2(len(train_data))))\n",
        "train_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n",
        "\n",
        "bins = train_data.bins.to_numpy()\n",
        "target = train_data.target.to_numpy()\n",
        "\n",
        "def rmse_score(y_true,y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true,y_pred))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3reNwiO7HzR",
        "outputId": "aa5c032c-6522-4831-bbde-27dea9d74e84"
      },
      "source": [
        "num_bins"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ZXP8VtQX2AFa",
        "outputId": "5ec915e4-ebf6-49e7-843b-ce9c948c2b1c"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url_legal</th>\n",
              "      <th>license</th>\n",
              "      <th>excerpt</th>\n",
              "      <th>target</th>\n",
              "      <th>standard_error</th>\n",
              "      <th>bins</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c12129c31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When the young people returned to the ballroom...</td>\n",
              "      <td>-0.340259</td>\n",
              "      <td>0.464009</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>85aa80a4c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
              "      <td>-0.315372</td>\n",
              "      <td>0.480805</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b69ac6792</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>As Roger had predicted, the snow departed as q...</td>\n",
              "      <td>-0.580118</td>\n",
              "      <td>0.476676</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dd1000b26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>And outside before the palace a great garden w...</td>\n",
              "      <td>-1.054013</td>\n",
              "      <td>0.450007</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37c1b32fb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Once upon a time there were Three Bears who li...</td>\n",
              "      <td>0.247197</td>\n",
              "      <td>0.510845</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id url_legal license  ...    target  standard_error  bins\n",
              "0  c12129c31       NaN     NaN  ... -0.340259        0.464009     7\n",
              "1  85aa80a4c       NaN     NaN  ... -0.315372        0.480805     7\n",
              "2  b69ac6792       NaN     NaN  ... -0.580118        0.476676     6\n",
              "3  dd1000b26       NaN     NaN  ... -1.054013        0.450007     5\n",
              "4  37c1b32fb       NaN     NaN  ...  0.247197        0.510845     8\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n-4PW8y2A-6",
        "outputId": "11e96cef-744a-4286-be60-ccb36b02c163"
      },
      "source": [
        "target"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.34025912, -0.31537234, -0.58011797, ...,  0.25520938,\n",
              "       -0.21527918,  0.30077875])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S93r1Y_h2Dwd"
      },
      "source": [
        "config = {\n",
        "    'lr': 2e-5,\n",
        "    'wd':0.01,\n",
        "    'batch_size':16,\n",
        "    'valid_step':10,\n",
        "    'max_len':256,\n",
        "    'epochs':3,\n",
        "    'nfolds':5,\n",
        "    'seed':42,\n",
        "    'model_path':f'{DATASET_PATH}/clrp_roberta_base', # Mention the Model path \n",
        "}\n",
        "\n",
        "for i in range(config['nfolds']):\n",
        "    os.makedirs(f'model{i}',exist_ok=True)\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONASSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(seed=config['seed'])\n",
        "\n",
        "train_data['Fold'] = -1\n",
        "kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n",
        "for k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=bins)):\n",
        "    train_data.loc[valid_idx,'Fold'] = k"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "IPa16fqT7_og",
        "outputId": "f8823d45-c350-4ee0-c4ac-ee77feeb8c25"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url_legal</th>\n",
              "      <th>license</th>\n",
              "      <th>excerpt</th>\n",
              "      <th>target</th>\n",
              "      <th>standard_error</th>\n",
              "      <th>bins</th>\n",
              "      <th>Fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c12129c31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When the young people returned to the ballroom...</td>\n",
              "      <td>-0.340259</td>\n",
              "      <td>0.464009</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>85aa80a4c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
              "      <td>-0.315372</td>\n",
              "      <td>0.480805</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b69ac6792</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>As Roger had predicted, the snow departed as q...</td>\n",
              "      <td>-0.580118</td>\n",
              "      <td>0.476676</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dd1000b26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>And outside before the palace a great garden w...</td>\n",
              "      <td>-1.054013</td>\n",
              "      <td>0.450007</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37c1b32fb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Once upon a time there were Three Bears who li...</td>\n",
              "      <td>0.247197</td>\n",
              "      <td>0.510845</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id url_legal license  ... standard_error  bins  Fold\n",
              "0  c12129c31       NaN     NaN  ...       0.464009     7     0\n",
              "1  85aa80a4c       NaN     NaN  ...       0.480805     7     2\n",
              "2  b69ac6792       NaN     NaN  ...       0.476676     6     3\n",
              "3  dd1000b26       NaN     NaN  ...       0.450007     5     2\n",
              "4  37c1b32fb       NaN     NaN  ...       0.510845     8     1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "xzQa_Ce88z8s",
        "outputId": "5103944f-007e-490a-de87-bb35e04c6a77"
      },
      "source": [
        "plt.figure(dpi=100)\n",
        "sns.countplot(train_data.bins);"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqQXb9h788eq"
      },
      "source": [
        "class CLRPDataset(Dataset):\n",
        "    def __init__(self,df,tokenizer,max_len=128):\n",
        "        self.excerpt = df['excerpt'].to_numpy()\n",
        "        self.targets = df['target'].to_numpy()\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        encode = self.tokenizer(self.excerpt[idx],\n",
        "                                return_tensors='pt',\n",
        "                                max_length=self.max_len,\n",
        "                                padding='max_length',\n",
        "                                truncation=True)\n",
        "        \n",
        "        target = torch.tensor(self.targets[idx],dtype=torch.float) \n",
        "        return encode, target\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.excerpt)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiPCJLha9xN_"
      },
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, in_features, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.middle_features = hidden_dim\n",
        "        self.W = nn.Linear(in_features, hidden_dim)\n",
        "        self.V = nn.Linear(hidden_dim, 1)\n",
        "        self.out_features = hidden_dim\n",
        "\n",
        "    def forward(self, features):\n",
        "        att = torch.tanh(self.W(features))\n",
        "        score = self.V(att)\n",
        "        attention_weights = torch.softmax(score, dim=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "\n",
        "        return context_vector"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GE7aVsN-Fb0"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self,path):\n",
        "        super(Model,self).__init__()\n",
        "        self.roberta = AutoModel.from_pretrained(path)  \n",
        "        self.config = AutoConfig.from_pretrained(path)\n",
        "        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear = nn.Linear(self.config.hidden_size,1)\n",
        "\n",
        "    def forward(self,**xb):\n",
        "        x = self.roberta(**xb)[0]\n",
        "        x = self.head(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM6VgWXO-KMR"
      },
      "source": [
        "def run(fold,verbose=True):\n",
        "    \n",
        "    def loss_fn(outputs,targets):\n",
        "        outputs = outputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        return torch.sqrt(nn.MSELoss()(outputs,targets))\n",
        "    \n",
        "    def train_and_evaluate_loop(train_loader,valid_loader,model,loss_fn,optimizer,epoch,fold,best_loss,valid_step=10,lr_scheduler=None):\n",
        "        train_loss = 0\n",
        "        for i, (inputs1,targets1) in enumerate(train_loader):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            inputs1 = {key:val.reshape(val.shape[0],-1) for key,val in inputs1.items()}\n",
        "            outputs1 = model(**inputs1)\n",
        "            loss1 = loss_fn(outputs1,targets1)\n",
        "            loss1.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss1.item()\n",
        "            \n",
        "            if lr_scheduler:\n",
        "                lr_scheduler.step()\n",
        "            \n",
        "            #evaluating for every valid_step\n",
        "            if (i % valid_step == 0) or ((i + 1) == len(train_loader)):\n",
        "                model.eval()\n",
        "                valid_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for j, (inputs2,targets2) in enumerate(valid_loader):\n",
        "                        inputs2 = {key:val.reshape(val.shape[0],-1) for key,val in inputs2.items()}\n",
        "                        outputs2 = model(**inputs2)\n",
        "                        loss2 = loss_fn(outputs2,targets2)\n",
        "                        valid_loss += loss2.item()\n",
        "                     \n",
        "                    valid_loss /= len(valid_loader)\n",
        "                    if valid_loss <= best_loss:\n",
        "                        if verbose:\n",
        "                            print(f\"epoch:{epoch} | Train Loss:{train_loss/(i+1)} | Validation loss:{valid_loss}\")\n",
        "                            print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n",
        "\n",
        "                        best_loss = valid_loss\n",
        "                        torch.save(model.state_dict(),f'./model{fold}/model{fold}.bin')\n",
        "                        tokenizer.save_pretrained(f'./model{fold}')\n",
        "                        \n",
        "        return best_loss\n",
        "    \n",
        "    accelerator = Accelerator()\n",
        "    print(f\"{accelerator.device} is used\")\n",
        "    \n",
        "    x_train,x_valid = train_data.query(f\"Fold != {fold}\"),train_data.query(f\"Fold == {fold}\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['model_path'])\n",
        "    model = Model(config['model_path'])\n",
        "\n",
        "    train_ds = CLRPDataset(x_train,tokenizer,config['max_len'])\n",
        "    train_dl = DataLoader(train_ds,\n",
        "                        batch_size = config[\"batch_size\"],\n",
        "                        shuffle=True,\n",
        "                        num_workers = 4,\n",
        "                        pin_memory=True,\n",
        "                        drop_last=False)\n",
        "\n",
        "    valid_ds = CLRPDataset(x_valid,tokenizer,config['max_len'])\n",
        "    valid_dl = DataLoader(valid_ds,\n",
        "                        batch_size = config[\"batch_size\"],\n",
        "                        shuffle=False,\n",
        "                        num_workers = 4,\n",
        "                        pin_memory=True,\n",
        "                        drop_last=False)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n",
        "    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= 10 * len(train_dl))\n",
        "\n",
        "    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n",
        "\n",
        "    print(f\"Fold: {fold}\")\n",
        "    best_loss = 9999\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        print(f\"Epoch Started:{epoch}\")\n",
        "        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,loss_fn,\n",
        "                                            optimizer,epoch,fold,best_loss,\n",
        "                                            valid_step=config['valid_step'],lr_scheduler=lr_scheduler)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS6E3SSZ-cIL",
        "outputId": "1eb927f6-b3d1-46a6-bc83-d1577f3ee4fc"
      },
      "source": [
        "for f in range(config['nfolds']):\n",
        "    run(f)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/My Drive/Data Science/data/commonlitreadabilityprize//clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/My Drive/Data Science/data/commonlitreadabilityprize//clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 0\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.1197649240493774 | Validation loss:1.175058947669135\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.175058947669135\u001b[0m\n",
            "epoch:0 | Train Loss:1.0996358394622803 | Validation loss:0.8710224098629422\n",
            "\u001b[32mValidation loss Decreased from 1.175058947669135 to 0.8710224098629422\u001b[0m\n",
            "epoch:0 | Train Loss:0.936041724114191 | Validation loss:0.7423608799775442\n",
            "\u001b[32mValidation loss Decreased from 0.8710224098629422 to 0.7423608799775442\u001b[0m\n",
            "epoch:0 | Train Loss:0.8414164108614768 | Validation loss:0.7125389642185636\n",
            "\u001b[32mValidation loss Decreased from 0.7423608799775442 to 0.7125389642185636\u001b[0m\n",
            "epoch:0 | Train Loss:0.7911472606892679 | Validation loss:0.611042882833216\n",
            "\u001b[32mValidation loss Decreased from 0.7125389642185636 to 0.611042882833216\u001b[0m\n",
            "epoch:0 | Train Loss:0.7608746627315146 | Validation loss:0.591389531062709\n",
            "\u001b[32mValidation loss Decreased from 0.611042882833216 to 0.591389531062709\u001b[0m\n",
            "epoch:0 | Train Loss:0.717272252580266 | Validation loss:0.5572404704160161\n",
            "\u001b[32mValidation loss Decreased from 0.591389531062709 to 0.5572404704160161\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.45239931074055756 | Validation loss:0.5544513099723392\n",
            "\u001b[32mValidation loss Decreased from 0.5572404704160161 to 0.5544513099723392\u001b[0m\n",
            "epoch:1 | Train Loss:0.46640494208277006 | Validation loss:0.5410972229308553\n",
            "\u001b[32mValidation loss Decreased from 0.5544513099723392 to 0.5410972229308553\u001b[0m\n",
            "Epoch Started:2\n",
            "epoch:2 | Train Loss:0.4196029874411496 | Validation loss:0.5229998909764819\n",
            "\u001b[32mValidation loss Decreased from 0.5410972229308553 to 0.5229998909764819\u001b[0m\n",
            "epoch:2 | Train Loss:0.38847233649389246 | Validation loss:0.5024758163425658\n",
            "\u001b[32mValidation loss Decreased from 0.5229998909764819 to 0.5024758163425658\u001b[0m\n",
            "epoch:2 | Train Loss:0.3929099406187351 | Validation loss:0.49796608669890297\n",
            "\u001b[32mValidation loss Decreased from 0.5024758163425658 to 0.49796608669890297\u001b[0m\n",
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/My Drive/Data Science/data/commonlitreadabilityprize//clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/My Drive/Data Science/data/commonlitreadabilityprize//clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 1\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.5247292518615723 | Validation loss:1.1474819895293977\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.1474819895293977\u001b[0m\n",
            "epoch:0 | Train Loss:1.0345157276500354 | Validation loss:0.791746167673005\n",
            "\u001b[32mValidation loss Decreased from 1.1474819895293977 to 0.791746167673005\u001b[0m\n",
            "epoch:0 | Train Loss:0.88670471736363 | Validation loss:0.7496366500854492\n",
            "\u001b[32mValidation loss Decreased from 0.791746167673005 to 0.7496366500854492\u001b[0m\n",
            "epoch:0 | Train Loss:0.817385575463695 | Validation loss:0.6563697730501493\n",
            "\u001b[32mValidation loss Decreased from 0.7496366500854492 to 0.6563697730501493\u001b[0m\n",
            "epoch:0 | Train Loss:0.7749397115009588 | Validation loss:0.6551410688294305\n",
            "\u001b[32mValidation loss Decreased from 0.6563697730501493 to 0.6551410688294305\u001b[0m\n",
            "epoch:0 | Train Loss:0.7155367311884145 | Validation loss:0.5971210789349344\n",
            "\u001b[32mValidation loss Decreased from 0.6551410688294305 to 0.5971210789349344\u001b[0m\n",
            "epoch:0 | Train Loss:0.690253110418857 | Validation loss:0.5847564968797896\n",
            "\u001b[32mValidation loss Decreased from 0.5971210789349344 to 0.5847564968797896\u001b[0m\n",
            "epoch:0 | Train Loss:0.6536336659204842 | Validation loss:0.5506478647391001\n",
            "\u001b[32mValidation loss Decreased from 0.5847564968797896 to 0.5506478647391001\u001b[0m\n",
            "epoch:0 | Train Loss:0.6346984596652839 | Validation loss:0.5504190739658144\n",
            "\u001b[32mValidation loss Decreased from 0.5506478647391001 to 0.5504190739658144\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.4722400954071905 | Validation loss:0.5417407924930254\n",
            "\u001b[32mValidation loss Decreased from 0.5504190739658144 to 0.5417407924930254\u001b[0m\n",
            "epoch:1 | Train Loss:0.4758911033471425 | Validation loss:0.5362146066294776\n",
            "\u001b[32mValidation loss Decreased from 0.5417407924930254 to 0.5362146066294776\u001b[0m\n",
            "Epoch Started:2\n",
            "epoch:2 | Train Loss:0.36451221593454775 | Validation loss:0.518190455933412\n",
            "\u001b[32mValidation loss Decreased from 0.5362146066294776 to 0.518190455933412\u001b[0m\n",
            "epoch:2 | Train Loss:0.3615040009627577 | Validation loss:0.5162394104732407\n",
            "\u001b[32mValidation loss Decreased from 0.518190455933412 to 0.5162394104732407\u001b[0m\n",
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/My Drive/Data Science/data/commonlitreadabilityprize//clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/My Drive/Data Science/data/commonlitreadabilityprize//clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 2\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.1979899406433105 | Validation loss:1.0941363374392192\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.0941363374392192\u001b[0m\n",
            "epoch:0 | Train Loss:0.9811591072516008 | Validation loss:0.8574089051948653\n",
            "\u001b[32mValidation loss Decreased from 1.0941363374392192 to 0.8574089051948653\u001b[0m\n",
            "epoch:0 | Train Loss:0.8485562589860731 | Validation loss:0.7263485714793205\n",
            "\u001b[32mValidation loss Decreased from 0.8574089051948653 to 0.7263485714793205\u001b[0m\n",
            "epoch:0 | Train Loss:0.814995008270915 | Validation loss:0.6696610003709793\n",
            "\u001b[32mValidation loss Decreased from 0.7263485714793205 to 0.6696610003709793\u001b[0m\n",
            "epoch:0 | Train Loss:0.7807160364646538 | Validation loss:0.6360939376884036\n",
            "\u001b[32mValidation loss Decreased from 0.6696610003709793 to 0.6360939376884036\u001b[0m\n",
            "epoch:0 | Train Loss:0.7572264519871258 | Validation loss:0.6315082808335623\n",
            "\u001b[32mValidation loss Decreased from 0.6360939376884036 to 0.6315082808335623\u001b[0m\n",
            "epoch:0 | Train Loss:0.7342596687901188 | Validation loss:0.5716784968972206\n",
            "\u001b[32mValidation loss Decreased from 0.6315082808335623 to 0.5716784968972206\u001b[0m\n",
            "epoch:0 | Train Loss:0.7144824030958576 | Validation loss:0.5576889109280374\n",
            "\u001b[32mValidation loss Decreased from 0.5716784968972206 to 0.5576889109280374\u001b[0m\n",
            "epoch:0 | Train Loss:0.6952979846315069 | Validation loss:0.5525161168641515\n",
            "\u001b[32mValidation loss Decreased from 0.5576889109280374 to 0.5525161168641515\u001b[0m\n",
            "epoch:0 | Train Loss:0.6819659973135089 | Validation loss:0.5473111909296777\n",
            "\u001b[32mValidation loss Decreased from 0.5525161168641515 to 0.5473111909296777\u001b[0m\n",
            "epoch:0 | Train Loss:0.6756548535179447 | Validation loss:0.5449450206425455\n",
            "\u001b[32mValidation loss Decreased from 0.5473111909296777 to 0.5449450206425455\u001b[0m\n",
            "epoch:0 | Train Loss:0.6587213478612561 | Validation loss:0.5287643712427881\n",
            "\u001b[32mValidation loss Decreased from 0.5449450206425455 to 0.5287643712427881\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.49578252705660736 | Validation loss:0.506121951672766\n",
            "\u001b[32mValidation loss Decreased from 0.5287643712427881 to 0.506121951672766\u001b[0m\n",
            "epoch:1 | Train Loss:0.479664240722303 | Validation loss:0.4999995132287343\n",
            "\u001b[32mValidation loss Decreased from 0.506121951672766 to 0.4999995132287343\u001b[0m\n",
            "Epoch Started:2\n",
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/My Drive/Data Science/data/commonlitreadabilityprize//clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/My Drive/Data Science/data/commonlitreadabilityprize//clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 3\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.1349283456802368 | Validation loss:1.207725602719519\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.207725602719519\u001b[0m\n",
            "epoch:0 | Train Loss:1.1081094362519004 | Validation loss:0.9174577477905486\n",
            "\u001b[32mValidation loss Decreased from 1.207725602719519 to 0.9174577477905486\u001b[0m\n",
            "epoch:0 | Train Loss:0.9712143057868594 | Validation loss:0.6808413888017336\n",
            "\u001b[32mValidation loss Decreased from 0.9174577477905486 to 0.6808413888017336\u001b[0m\n",
            "epoch:0 | Train Loss:0.8839635002997613 | Validation loss:0.5994292323788007\n",
            "\u001b[32mValidation loss Decreased from 0.6808413888017336 to 0.5994292323788007\u001b[0m\n",
            "epoch:0 | Train Loss:0.8313165481497602 | Validation loss:0.5788867763347096\n",
            "\u001b[32mValidation loss Decreased from 0.5994292323788007 to 0.5788867763347096\u001b[0m\n",
            "epoch:0 | Train Loss:0.7836327415997865 | Validation loss:0.5324643585417006\n",
            "\u001b[32mValidation loss Decreased from 0.5788867763347096 to 0.5324643585417006\u001b[0m\n",
            "epoch:0 | Train Loss:0.7604283665267515 | Validation loss:0.522263808382882\n",
            "\u001b[32mValidation loss Decreased from 0.5324643585417006 to 0.522263808382882\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.5261594653129578 | Validation loss:0.5208696946501732\n",
            "\u001b[32mValidation loss Decreased from 0.522263808382882 to 0.5208696946501732\u001b[0m\n",
            "epoch:1 | Train Loss:0.48434094763269614 | Validation loss:0.49442876709832084\n",
            "\u001b[32mValidation loss Decreased from 0.5208696946501732 to 0.49442876709832084\u001b[0m\n",
            "Epoch Started:2\n",
            "epoch:2 | Train Loss:0.3751881737108073 | Validation loss:0.46862227469682693\n",
            "\u001b[32mValidation loss Decreased from 0.49442876709832084 to 0.46862227469682693\u001b[0m\n",
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/My Drive/Data Science/data/commonlitreadabilityprize//clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/My Drive/Data Science/data/commonlitreadabilityprize//clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 4\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.0219272375106812 | Validation loss:1.0334652993414137\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.0334652993414137\u001b[0m\n",
            "epoch:0 | Train Loss:0.9771076332439076 | Validation loss:0.7710481277770467\n",
            "\u001b[32mValidation loss Decreased from 1.0334652993414137 to 0.7710481277770467\u001b[0m\n",
            "epoch:0 | Train Loss:0.8600724112419855 | Validation loss:0.6730372649100091\n",
            "\u001b[32mValidation loss Decreased from 0.7710481277770467 to 0.6730372649100091\u001b[0m\n",
            "epoch:0 | Train Loss:0.8197273919659276 | Validation loss:0.6615686573916011\n",
            "\u001b[32mValidation loss Decreased from 0.6730372649100091 to 0.6615686573916011\u001b[0m\n",
            "epoch:0 | Train Loss:0.7197617741881824 | Validation loss:0.6399387700690163\n",
            "\u001b[32mValidation loss Decreased from 0.6615686573916011 to 0.6399387700690163\u001b[0m\n",
            "epoch:0 | Train Loss:0.672198485619951 | Validation loss:0.5368737586670451\n",
            "\u001b[32mValidation loss Decreased from 0.6399387700690163 to 0.5368737586670451\u001b[0m\n",
            "epoch:0 | Train Loss:0.6606603938717026 | Validation loss:0.5345427435305383\n",
            "\u001b[32mValidation loss Decreased from 0.5368737586670451 to 0.5345427435305383\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.42480048679170157 | Validation loss:0.5271391777528657\n",
            "\u001b[32mValidation loss Decreased from 0.5345427435305383 to 0.5271391777528657\u001b[0m\n",
            "epoch:1 | Train Loss:0.4599554311853271 | Validation loss:0.5014400664303038\n",
            "\u001b[32mValidation loss Decreased from 0.5271391777528657 to 0.5014400664303038\u001b[0m\n",
            "Epoch Started:2\n",
            "epoch:2 | Train Loss:0.36477140308791445 | Validation loss:0.49276768995655906\n",
            "\u001b[32mValidation loss Decreased from 0.5014400664303038 to 0.49276768995655906\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2rKO0sP-hpd",
        "outputId": "5606a008-bce6-4662-858a-420b14d2ff13"
      },
      "source": [
        "!zip -r /content/model0.zip /content/model0/"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/model0/ (stored 0%)\n",
            "  adding: content/model0/tokenizer_config.json (deflated 45%)\n",
            "  adding: content/model0/model0.bin (deflated 7%)\n",
            "  adding: content/model0/special_tokens_map.json (deflated 50%)\n",
            "  adding: content/model0/tokenizer.json (deflated 59%)\n",
            "  adding: content/model0/vocab.json (deflated 59%)\n",
            "  adding: content/model0/merges.txt (deflated 53%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7zY2alELfmx",
        "outputId": "7af6e974-bf99-4bbb-eb91-822c37dc934a"
      },
      "source": [
        "!zip -r /content/model1.zip /content/model1/"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/model1/ (stored 0%)\n",
            "  adding: content/model1/tokenizer_config.json (deflated 45%)\n",
            "  adding: content/model1/special_tokens_map.json (deflated 50%)\n",
            "  adding: content/model1/tokenizer.json (deflated 59%)\n",
            "  adding: content/model1/vocab.json (deflated 59%)\n",
            "  adding: content/model1/model1.bin (deflated 7%)\n",
            "  adding: content/model1/merges.txt (deflated 53%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKIfWFwELi5l",
        "outputId": "19e428f7-0d8d-4dad-d2e3-fb9dbacd02c5"
      },
      "source": [
        "!zip -r /content/model2.zip /content/model2/"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/model2/ (stored 0%)\n",
            "  adding: content/model2/tokenizer_config.json (deflated 45%)\n",
            "  adding: content/model2/model2.bin (deflated 7%)\n",
            "  adding: content/model2/special_tokens_map.json (deflated 50%)\n",
            "  adding: content/model2/tokenizer.json (deflated 59%)\n",
            "  adding: content/model2/vocab.json (deflated 59%)\n",
            "  adding: content/model2/merges.txt (deflated 53%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqLn8m8SLlbT",
        "outputId": "13018baa-c492-4fdd-c9e3-99b0c0694ad7"
      },
      "source": [
        "!zip -r /content/model3.zip /content/model3/"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/model3/ (stored 0%)\n",
            "  adding: content/model3/model3.bin (deflated 7%)\n",
            "  adding: content/model3/tokenizer_config.json (deflated 45%)\n",
            "  adding: content/model3/special_tokens_map.json (deflated 50%)\n",
            "  adding: content/model3/tokenizer.json (deflated 59%)\n",
            "  adding: content/model3/vocab.json (deflated 59%)\n",
            "  adding: content/model3/merges.txt (deflated 53%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5LBppIPLoCT",
        "outputId": "55567808-fc86-403a-9427-c72dd79f6b77"
      },
      "source": [
        "!zip -r /content/model4.zip /content/model4/"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/model4/ (stored 0%)\n",
            "  adding: content/model4/tokenizer_config.json (deflated 45%)\n",
            "  adding: content/model4/special_tokens_map.json (deflated 50%)\n",
            "  adding: content/model4/tokenizer.json (deflated 59%)\n",
            "  adding: content/model4/vocab.json (deflated 59%)\n",
            "  adding: content/model4/model4.bin (deflated 7%)\n",
            "  adding: content/model4/merges.txt (deflated 53%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "e7B7ZHwKLqYa",
        "outputId": "e2960196-d751-4db9-b506-b91164618ddb"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/model0.zip\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_15de578b-9163-4462-92a7-018759940300\", \"model0.zip\", 466052688)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho-bssbWLxEO"
      },
      "source": [
        "\n",
        "files.download(\"/content/model1.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xlmiLkb_LyTF",
        "outputId": "60d057be-ac44-4840-883e-a5ff6a0d9528"
      },
      "source": [
        "\n",
        "files.download(\"/content/model2.zip\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5aec338e-b5f6-40ea-98df-e8c89a6f77c1\", \"model2.zip\", 466057707)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u1HTg_ZL0X-"
      },
      "source": [
        "\n",
        "files.download(\"/content/model3.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "KgXod6BaL1kJ",
        "outputId": "4429e5e9-0f47-4ebb-f32e-c43c6fb9c930"
      },
      "source": [
        "\n",
        "files.download(\"/content/model4.zip\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_004fa8f7-a5b2-4d8a-92a6-4cdefda36aec\", \"model4.zip\", 466051201)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzjfFtaDkKkx"
      },
      "source": [
        "|"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}