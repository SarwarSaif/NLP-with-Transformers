{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline-sentiment-analysis-using-prebuilt-flair-distilbert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPULCAXKnK+oxpWKSo/XHc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EmHB9dzZZB-"
      },
      "source": [
        "# Introduction\n",
        "Sentiment analysis from text is an NLP task where we classify sentiments of sentences as Positives, Negatives or Neutral. The number of categories might differ. \n",
        "Example: \n",
        "1. Positive: \"GBP rallies after stronger than ever economic performance.\"\n",
        "2. Negative: \"GBP slips on fear of a divided economy.\"\n",
        "3. Neutral: \"GBP is one of several European currencies.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEy2F7ChZY5y"
      },
      "source": [
        "The typical flow of a **Sentiment classification model using Transformers** consists of four steps:\n",
        "1. Raw text is Tokenized. A tokenizer usually converts words into a list of numerical ids. \n",
        "2. Token IDs fed into the Model. \n",
        "  With Transformer model, the first layer will be Embedding Layer and our token ids will be passed through this layer. And we will get a Set of Word Vectors from the output of Embedding layer. \n",
        "  - This is the very first layer of most Transformer Models.\n",
        "3. The model typically outputs a set of Activations. So we need to feed these Output Activations in our Transformer Heads, and then our Transformer Heads will convert these activations into a Set of Probabilities. And Softmax is applied onto the activations after they are processed by the linear layers, the Sum of the Probabilities will be 1.\n",
        "4. Argmax is applied at the end to get the Class with Highest Probability.\n",
        "\n",
        "Note: It can be possible to change the Number of Output Classes to fit our number of desired classes in the head module. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgE6_emLuMfa"
      },
      "source": [
        "# First Introduction to Transformers using Prebuilt Flair Models\n",
        "Flair Model: https://github.com/flairNLP/flair\n",
        "Why Flair?\n",
        "\n",
        "It's a - \n",
        "\n",
        "> - A powerful NLP library. Flair allows you to apply the state-of-the-art natural language processing (NLP) models to anyone's text, such as named entity recognition (NER), part-of-speech tagging (PoS), special support for biomedical data, sense disambiguation and classification, with support for a rapidly growing number of languages.\n",
        "> - A text embedding library. Flair has simple interfaces that allow anyone to use and combine different word and document embeddings, including their proposed Flair embeddings, BERT embeddings and ELMo embeddings.\n",
        "> - A PyTorch NLP framework. This framework builds directly on PyTorch, making it easy to train anyone's own models and experiment with new approaches using Flair embeddings and classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn_stWT0Fq7M"
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrtrCYyQugJD"
      },
      "source": [
        "## Steps:\n",
        "> 1. Initialize the Model.\n",
        "> 2. Tokenizing.\n",
        "> 3. Process with Model.\n",
        "> 4. Format the Outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuFru18Duaze"
      },
      "source": [
        "import flair\n",
        "# Download and Intitialize the Flair Model\n",
        "model = flair.models.TextClassifier.load('en-sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQXGh6Q4wTXo",
        "outputId": "9aea75e9-03ee-4afe-c292-51ef4a6b91eb"
      },
      "source": [
        "text = \"Do you love me? I don't think so. If you did, you wouldn't have done this to me.\"\n",
        "\n",
        "sentence = flair.data.Sentence(text)\n",
        "\n",
        "sentence"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentence: \"Do you love me ? I do n't think so . If you did , you would n't have done this to me .\"   [− Tokens: 24]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "COhRZ8mryRFC",
        "outputId": "7ace0928-f235-45aa-e0aa-951ac3eeb2a6"
      },
      "source": [
        "sentence.to_tokenized_string()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Do you love me ? I do n't think so . If you did , you would n't have done this to me .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhTN-Jhb0VG9"
      },
      "source": [
        "### Process the Tokenized Inputs through out the DistilBERT Classifier\n",
        "[DistilBERT](https://arxiv.org/pdf/1910.01108.pdf) is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased , runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eokBtyxnzo4A"
      },
      "source": [
        "model.predict(sentence)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fU3zDBH0HVZ",
        "outputId": "8fd95568-3e64-4059-85ab-15b1233b3ae0"
      },
      "source": [
        "sentence"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentence: \"Do you love me ? I do n't think so . If you did , you would n't have done this to me .\"   [− Tokens: 24  − Sentence-Labels: {'label': [NEGATIVE (0.8097)]}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsp54QOk0KGO",
        "outputId": "d228be92-a285-4f73-b802-dadcbdfe49ac"
      },
      "source": [
        "sentence.get_labels()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[NEGATIVE (0.8097)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3leKL3B07f3",
        "outputId": "32f144b3-9fb5-411c-e89a-86f2965adc1e"
      },
      "source": [
        "sentence.get_labels()[0].value, sentence.get_labels()[0].score"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('NEGATIVE', 0.8096855878829956)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPSQbwpM1LlN",
        "outputId": "c2e1d072-98af-4d32-a40e-fa7a9eeb82ef"
      },
      "source": [
        "sentence.labels[0].value, sentence.labels[0].score"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('NEGATIVE', 0.8096855878829956)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU52OJIB1aaJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}